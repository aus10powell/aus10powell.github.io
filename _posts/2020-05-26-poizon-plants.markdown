---
layout: splash
title:  "Poizon Plants"
date:   2020-05-26
comments: true
image: /assets/images/poizon_plants/poizon_plants_app.jpg
categories: Computer Vision AI Machine-Learning Poison-Oak Plants
description: "An attempt to make a vision classifier for poison oak practical."
---

**An app to easily identify poison oak**

# Poizon Plants [Blog in development]
![image](/assets/images/poizon_plants/poizon_plants_app.jpg){: style="float: left; margin-right: 1em;"}

### Intro/Motivation
I have gotten poison oak multiple times. While it is debated whether continued exposure to the oil found on the plant that causes the allergic reaction gets worse over time or better, the fact remains it not fun. Particularly in certain areas of your body.

Apparently, while some people do not have an allergic reaction, there is no proof that you won't develop a rash with continued exposure per [American Osteopathic College of Dermatology](https://www.aocd.org/page/PoisonIvyDermatiti) (among other sources). So basically, no one is really safe.

If you don't live in California, this might not really matter as much. While Urushiol (the oil on poison oak causing the allergic reaction) can be found on plants all over the world, it seems to really love the North American coast. Those of us living in California for more than a few years are likely familiar with, or at least have heard of, poison oak. But even people native to California (much less one of the millions of tourists) have trouble identifying the plant if is not in it's signature glowing, oily red.

Given that mobile phones are ubiquitous even when out enjoying nature, a poison oak app seemed like useful project and learning opportunity.

I had no interest in duplicating effort for something that already had a solution, so I did a little research to see if there were any existing solutions for this niche challenge. Interestingly enough, there were already a few apps on the iOS store that were simple classification apps like the one I proposed. When I tested these against my gold-standard dataset, they had very similar performance the model that had been trained on Google Images. Go figure.

### Cold-start problem
![image](/assets/images/poizon_plants/classic_red_poison_oak.jpg "Classic Red Poison Oak Bush"){: style="float: right; margin-right: 1em;"}

Perhaps the obvious place to start for labeled images of poison oak was Google. The easiest method I've found so far to do this is a Chrome extension [here](https://chrome.google.com/webstore/detail/imageye-image-downloader/agionbommeaifngbhincahgmoflcikhm?hl=en).


As far as the semantics of the search ("poison oak", "poison oak bush", "poison oak autumn", etc.), I focused on making searches that got images of the plant under different seasons. This is the first place where Google revealed it's bias since the major season captured for poison oak seemed to be when it was at its most obvious and red. For the "not poison" images, I tried to cover a broad flora that might exist geographically with poison oak and especially on plants whose leaves I thought would confuse the average hiker. **This initial dataset netted me about 3k images** in total after cleaning out the expected garbage images. Time to baseline.

Just to get any idea of what kind of signal existed in this dataset, I trained a Resnet50 on a 80/20 random split of the data. After a little tweaking, I got above 98% accuracy (on validation set). This was too good to be true and a red flag that I had a lot of bias going on. My take is that Google is providing data generated by a similar model that I trained.

The only solution was to gather my own data. The benefit of acquiring data this way is that I would be:
* 1) labeling on the go
* 2) refining the concept-space of what constituted a reasonable recognition of an image containing poison oak since this could include multiple plants.
* 3) have a reason to go on more backpacking trips

#### Labeling Process

#### Choice of Cut-off Probability
This is a choice which I've seen discussed very rarely for a binary classification problem (at least in workshops/tutorials/books) but can have great practical implications for the end-user. It generally assumed that the cut-off for a binary classifier is 50/50 for deciding whether to bin the output of the softmax as a 1 or 0. I chose to model the app as a degree of certainty that a given image was poison oak based on feedback from different people I had test the app out. In this case, binning the probabilities into categories such as "possibly poison oak", "definitley poison oak", etc. provided better intuition to the user that a 60% vs a 80% probability.
    <figure class="half">
    <a href="/assets/images/poizon_plants/IMG_2268.jpg"><img src="/assets/images/poizon_plants/probability_cutoff_not_poison.jpg" style="width:100%;height:90%"></a>
    <a href="/assets/images/poizon_plants/IMG_3408.jpg"><img src="/assets/images/poizon_plants/probability_cutoff_is_poison.jpg" style="width:100%;height:90%"></a>
    <figcaption>Probability Cut-off.</figcaption>
    </figure>

## Modeling

### Training
* Model Tracking: Utilization of Weights & Biases ![link]https://www.wandb.com/() was very useful when iterating over different models. It even has an image viewer so that you can peek at how you model is predicting example images while it is training.
* ![Hyper-parameter Sweeps](https://www.wandb.com/articles/introduction-hyperparameter-sweeps)
* **Choice of architecture:**:
  * Due to wanting to have a very controlled way of dealing with outliers, i.e. more distant pictures of a bush of poison oak it was necessary to use both dropout and L2 regularization (![Here is a good blog post on this topic](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)). L2 was a reasonable choice for regularization since I wanted to smooth more for those outliers (as opposed to L1 which does not penalize as strongly). The reason for this is in the figure below:
    * We don't care the THIS image has a probability of 92% vs 99%
    * We DO care if THIS image has a probability of 60% vs 5%:


### Converting Tensorflow to TF-Lite
* **Why:**
  * CoreML is not as well supported
  * TF-Lite could also be used on Android devices
  * Optimizations are offered for TF Lite, however with my testing (TF V2.3.0):
    * Default
    * Latency
    * Size

```python
from tensorflow.keras.models import load_model
from tensorflow.lite.TFLiteConverter import from_keras_model

# Get model object from my MobileNet
model = train_model()

# Load model (I had saved as a .model file and it worked fine)
model = load_model(model_path, custom_objects=dependencies,compile=True)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

#### Other Training Lessons Learned
* Initially, I made all all layers trainable which resulted in a extremely spikey, but generally decreasing validation loss but only after huge number of 250 epochs. After running some experiments, I found a much steader decrease of validation loss at training only the last 20 layers which resulted in


#### Choice of object detection vs classification task
When thinking about how to make the app practical for hikers vs what was practical for modeling, it seemed like there were 2 choices: focus on the object detection task or a classification task.

![image](/assets/images/poizon_plants/object_dection_bush.jpg "Detecting Poison Oak Bush")

Practically speaking, it seems more useful that a person would pull out their phone and sweep the camera over a range of plants. Looking ahead, this would involve a lot of manual labeling of bounding boxes and too much work. The project could always be easily extended to include this scope also. So classification it was.

### Getting to > 90% F1 score
This goal was set both for practical reasons (having a classifier app that was a satisfying product), as well as a mental milestone for "I understand training an image classifier". There is potentially a lot of learning to be had between 87% F1 score that I struggled with for a while and 90% F1-score.

#### Other
* iOS uses 299x299 images so it was important to optimize on this image size during model development

#### Analyzing images of highest loss


## Important Lessons:
* **Focus on your highest-loss images to understand true performance**

This was **useful both for cleaning bad/poor quality images** as well as finding areas where potentially you might need to break out a new class, e.g. "Unknown bush"

* **Think your data is well-labeled? Think again...and again:**
    * Despite having gone through and labeled, by hand, thousands of images, examining images with top log-loss showed the my human error. I think one reason for this is during the labeling process, I had additional context for "yes this is poison oak" due to having walked by a large bush of poison oak already. When looking only at the photo that was taken with no additional context, which is what the neural net is doing, it was not clear to my human eye.
* **How do you know when your data is enough?**
Particularly for the practicality of my problem where a region taken with your phone "may contain poison oak", it wasn't immediately clear if I had pictures with sufficient variation. An example of when this issue first surfaced was when a plant that was not poison oak but was reddish immedietly caused trouble for the algorithm. It was a bit of a Catch-22, because while red is such strong indicator (for humans and neural nets) for being a sumac plant, *red is definitly **not*** a rule for a plant being poison oak.
    <figure class="half">
    <a href="/assets/images/poizon_plants/IMG_2268.jpg"><img src="/assets/images/poizon_plants/IMG_2268.jpg" style="width:100%;height:90%"></a>
    <a href="/assets/images/poizon_plants/IMG_3408.jpg"><img src="/assets/images/poizon_plants/IMG_3408.jpg" style="width:100%;height:90%"></a>
    <figcaption>Strong red color can be indicative of poison oak but also a strong false positive.</figcaption>
    </figure>
* **Training**
Augmentation is quit helpful for this usecase. Shift and rotation especially. But I came across a moment when my validation accuracy would reach a certain point around 70% where it wouldn't get better; but it wouldn't get worse either. This is due to the rookie mistake of augmenting the validation data as well as the training data.

* **A fair comparison:**
    <figure class="half">
    <a href="/assets/images/poizon_plants/IMG_4273..jpg"><img src="/assets/images/poizon_plants/IMG_4273.jpg" style="width:100%;height:90%"></a>
    <a href="/assets/images/poizon_plants/IMG_IMG_4274.jpg"><img src="/assets/images/poizon_plants/IMG_4274.jpg" style="width:100%;height:90%"></a>
     <figcaption>Contextual image information may often be a confounder.</figcaption>
    </figure>


### References
* **[Poizon Plant iOS app](https://apps.apple.com/us/app/poizon-plant/id1475980295 "Link to iOS App")**

### Papers I checked out
* [A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network](https://ieeexplore.ieee.org/abstract/document/4458016)
* [Plant Disease Detection Using Deep learning](https://arxiv.org/abs/2003.05379)
