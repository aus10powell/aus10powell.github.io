---
layout: single
title:  "2023 Papers"
date:   2023-01-01
comments: true
# image: /assets/images/poizon_plants/poizon_plants_app.jpg
categories: 2023 Papers 
description: "Papers Read in 2023"
keywords: research, ml, data-science, papers, nlp, audio, ai
show_date: true
words_per_minute: 300
---

**Papers Read in 2023**

## Audio
* [Self-supervised learning for infant cry analysis](https://arxiv.org/abs/2305.01578")
    * Self-supervised learning can be used to learn useful representations of infant cries from unlabeled data.
    * The self-supervised approach was able to achieve comparable performance to a supervised learning approach that was trained on a small amount of labeled data.
    * Self-supervised learning could be a valuable tool for developing new and improved infant cry analysis systems.
* [CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds](https://arxiv.org/abs/2305.00969")
    * The CryCeleb dataset is a large and diverse dataset of infant cry sounds.

## NLP
* [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
    * BART is a denoising autoencoder, which means that it is trained on a dataset of corrupted text. The corruptions can be simple, such as replacing words with random words, or more complex, such as removing words or sentences. BART is trained to reconstruct the original text from the corrupted text.
    * This training procedure helps BART to learn to represent the meaning of text, even when the text is corrupted. This makes BART well-suited for natural language generation, translation, and comprehension tasks.

### Large-Language Models
* [Parameter-Efficient Transfer Learning for NLP](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf)
* [RLHF: Reinforcement Learning from Human Feedback (A blog by Chip Huyen)](https://huyenchip.com/2023/05/02/rlhf.html)