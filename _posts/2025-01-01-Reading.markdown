---
layout: single
title: "2025 Technical Reading"
date: 2025-01-01
comments: true
# image: /assets/images/poizon_plants/poizon_plants_app.jpg
categories: 2025 Papers
description: "Technical articles, research papers, etc. read in 2025"
keywords: research, ml, data-science, papers, nlp, audio, ai
show_date: true
words_per_minute: 300
toc: true
toc_label: "Contents"
toc_icon: "book" # Optional
toc_sticky: true # Makes ToC scroll with page
---

**2025 Reading List**

## NLP

### LLMs

- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764): BitNet b1.58 is a new way to make AI language models use less memory and power by reducing each parameter to just three values: -1, 0, or 1. The model works as well as regular 16-bit models when it reaches 3 billion parameters, but uses about 3.5 times less memory and runs almost 3 times faster. At larger sizes (70 billion parameters), it runs even better - about 4 times faster and uses much less memory than standard models. The model saves a lot of energy too - using 71 times less power for its main calculations compared to regular models. Tests show it performs well on language tasks and can handle long training sessions (2 trillion tokens) with good results.

### Agents (LLM-based)

- [AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data](https://arxiv.org/pdf/2410.11531)
    > LLMs are powerful but can struggle with factual consistency and require complex queries. Traditional KG tools are difficult to use and require technical expertise. AGENTiGraph bridges the gap between LLMs and KGs. It uses a multi-agent system where each agent has a specific role, such as interpreting user intent or translating queries into graph operations. This allows for more natural language interaction with KGs and improved accuracy in tasks like question answering.
